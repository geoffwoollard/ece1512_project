{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Deep Learning \n",
    "## Neural Networks \n",
    "\n",
    "Start with Slide Show \n",
    "First we import some libraries: \n",
    "\n",
    "    keras.datasets mnist: The MNIST data set provided from Keras \n",
    "    matplotlib.pyplot: Visualization tool for observing the data\n",
    "    numpy: Mathematical Library that we will use to build a neural network from scratch\n",
    "    keras.layers, models, optimmizers: Keras library is built for making NN quick and easily \n",
    "    sklearn: Welld eveloped ML tool kit for data manipulation \n",
    "    pandas: a Dataframe library that caan be used for sattistical evaluations and organizing big data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop, Adadelta, Adam, SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to observe a common data set used for learning NN. \n",
    "The MNIST data set is a large data base of images of handwritten digits, with a corresponding label. \n",
    "We will load the data an observe a random example from out training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (60000, 28, 28)\n",
      "Target Shape: (60000,)\n",
      "Picture of 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAA7RJREFUeJzt3cFOYkEURVHK8P+/XE50QGIAQXhW7bWGPWki2bmJh4djznkC9vdx9AsA3kPsECF2iBA7RIgdIs7v/M/GGH71Dy825xw//bvLDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdog4H/0CCuacR7+ElxljHP0SuJPLDhFihwixQ4TYIULsECF2iDC9fdl5HnulI39uZr/fcdkhQuwQIXaIEDtEiB0ixA4RYocIOzvLurXx2+EvuewQIXaIEDtEiB0ixA4RYocIsUNEZmf3vHrPM+/5jhu9yw4RYocIsUOE2CFC7BAhdogQO0RkdvZbu+nKO/yOm/DpdOx7suOz8i47RIgdIsQOEWKHCLFDhNghQuwQkdnZWc/On404gssOEWKHCLFDhNghQuwQIXaIML3BA1Z8BNZlhwixQ4TYIULsECF2iBA7RIgdIuzsXzxOuR7v2e+47BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhOfZ73Tt2WnPTbMClx0ixA4RYocIsUOE2CFC7BAhdoiws7Mtn4245LJDhNghQuwQIXaIEDtEiB0iTG9/wJ8OZgUuO0SIHSLEDhFihwixQ4TYIULsEGFn38C1Hf/WZwB4zIo/V5cdIsQOEWKHCLFDhNghQuwQIXaIsLNv7tln6Vfck7/5HoFLLjtEiB0ixA4RYocIsUOE2CFC7BBhZ3+Dlb9XvrzT78ZlhwixQ4TYIULsECF2iBA7RJje/oGVp7lbVn7tu3HZIULsECF2iBA7RIgdIsQOEWKHCDv7Anbe4Xkflx0ixA4RYocIsUOE2CFC7BAhdoiws2/g2g5vg3/Mjl+B7bJDhNghQuwQIXaIEDtEiB0ixA4RdvbNPbsXr7zT77iVP8NlhwixQ4TYIULsECF2iBA7RIgdIuzsXGWr3ofLDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogYK/9JXuB+LjtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQ8QmP3FoRvKDrmgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(o_x_train, o_y_train), (o_x_test, o_y_test) = mnist.load_data()\n",
    "\n",
    "i = np.random.randint(0,o_x_train.shape[0])\n",
    "print('Data Shape: ' + str(o_x_train.shape))\n",
    "print('Target Shape: ' + str(o_y_train.shape))\n",
    "print('Picture of ' + str(o_y_train[i]))\n",
    "plt.imshow(o_x_train[i], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Pre-Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning of every experiment, we split the training data into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Training set images have this shape: (54000, 28, 28)\n",
      "The Training set labels have this shape: (54000,)\n",
      "The Validation set images have this shape: (6000, 28, 28)\n",
      "The Validation set labels have this shape: (6000,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(o_x_train, o_y_train,\n",
    "                                                  test_size=0.1,\n",
    "                                                  random_state=20)\n",
    "print('The Training set images have this shape: ' + str(x_train.shape))\n",
    "print('The Training set labels have this shape: ' + str(y_train.shape))\n",
    "print('The Validation set images have this shape: ' + str(x_val.shape))\n",
    "print('The Validation set labels have this shape: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start with the NN, we need to take our images os (28x28) pxels and flatten them into one dimensional vectors for the input. And we need to categorize the labels into a vector of zeros and ones to train the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Training set images have this shape: (54000, 784)\n",
      "The Training set labels have this shape: (54000, 10)\n",
      "The Validation set images have this shape: (6000, 784)\n",
      "The Validation set labels have this shape: (6000, 10)\n"
     ]
    }
   ],
   "source": [
    "y_train_targ = pd.get_dummies(y_train)\n",
    "y_val_targ = pd.get_dummies(y_val)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0],\n",
    "                          x_train.shape[1]*x_train.shape[2] )\n",
    "\n",
    "x_val = x_val.reshape(x_val.shape[0],\n",
    "                      x_val.shape[1] * x_val.shape[2])\n",
    "print('The Training set images have this shape: ' + str(x_train.shape))\n",
    "print('The Training set labels have this shape: ' + str(y_train_targ.shape))\n",
    "print('The Validation set images have this shape: ' + str(x_val.shape))\n",
    "print('The Validation set labels have this shape: ' + str(y_val_targ.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Building a NN the EASY way \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use keras which is the most popular library right now for developing NNs. \n",
    "\n",
    "This model has an input of the dimension of our data (784) and an output of (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1568)              1230880   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1568)              2460192   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                15690     \n",
      "=================================================================\n",
      "Total params: 3,706,762\n",
      "Trainable params: 3,706,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1568, input_dim=x_train.shape[1], activation='sigmoid'))\n",
    "model.add(Dense(1568, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=SGD(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Training\n",
    "\n",
    "Next we train the model using the keras fit function. Then at the end we can evaluate the model on the validation set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 15s 283us/step - loss: 2.6561 - categorical_accuracy: 0.1025\n",
      "6000/6000 [==============================] - 2s 269us/step\n",
      "\n",
      "categorical_accuracy: 11.12%\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train_targ.values, epochs=1, batch_size=x_train.shape[0])\n",
    "\n",
    "\n",
    "scores = model.evaluate(x_val, y_val_targ.values)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And Congradulations! You've built your first neural network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks from scratch \n",
    "\n",
    "To better understand the keras model that we just built, we will delve into building a neural network from scratch using Numpy.\n",
    "\n",
    "Here we are going to use some object oriented programming. The basicNN will be our class that will be built of our model. So we are going to go through this slowly. \n",
    "\n",
    "BUT!! Before we can build the class we need some pieces built first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid function is going to be the output of our hidden layers. Output Domain: [-1, 1]\n",
    "def sigmoid(s):\n",
    "    return 1/(1 + np.exp(-s))\n",
    "\n",
    "# Softmax will be the activation function for the output. Output Domain: [0,1]\n",
    "def softmax(s):\n",
    "    exps = np.exp(s-np.max(s,axis=1, keepdims=True))\n",
    "    return exps/np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "# For backpropagation algorithm,, we will need the derivative of our activation functions to update the weights. \n",
    "# therefore this is the derivative of the Sigmoid function withr espect to it's input \n",
    "def dSigmoid_ds(s):\n",
    "    return (1-sigmoid(s))*sigmoid(s)\n",
    "\n",
    "# This is our loss function. We will use cross entropy as we did in the keras model \n",
    "def cross_entropy(pred, true):\n",
    "    n_samples = true.shape[0]\n",
    "    res = pred - true\n",
    "    return res/n_samples\n",
    "\n",
    "# And finally we need a way to quantify the error. \n",
    "# This will take a predicted vecctor and ground truth vector and determine how correct the prediction is. \n",
    "def error(pred, true):\n",
    "    n_samples = true.shape[0]\n",
    "    logp = -np.log(pred[np.arange(n_samples), true.argmax(axis=1)])\n",
    "    loss = np.sum(logp)/(n_samples)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "class BasicNN:\n",
    "    def __init__(self, x, y):\n",
    "        ''' \n",
    "        This is the initializer for the class. \n",
    "        All we need as input for the initialization is the training data and the labels \n",
    "        '''\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        # Now we can declare how large we want the hidden layers to be. \n",
    "        # Lets keep the same structure as above \n",
    "        neurons = 1568\n",
    "        \n",
    "        # Then we need to decide on a learning rate. In the Keras SGD optimizer, this is 0.01\n",
    "        self.lr = 0.01\n",
    "        \n",
    "        # We will declare variables of the input and output shape\n",
    "        ip_dim = x.shape[1]\n",
    "        op_dim = y.shape[1]\n",
    "\n",
    "        # Finally we decide to Declare Weights and biases usign a random number generator \n",
    "        # wi is a matrix\n",
    "        # bi is a vector\n",
    "\n",
    "        self.w1 = np.random.randn(ip_dim, neurons)\n",
    "        self.b1 = np.zeros((1, neurons))\n",
    "\n",
    "        self.w2 = np.random.randn(neurons, neurons)\n",
    "        self.b2 = np.zeros((1, neurons))\n",
    "\n",
    "        self.w3 = np.random.randn(neurons, op_dim)\n",
    "        self.b3 = np.zeros((1,op_dim))\n",
    "\n",
    "    def feedforward(self):\n",
    "        '''\n",
    "        Feeds forward the data\n",
    "        This function will run a sample from the beginning to the end and give us our out put. \n",
    "        Remember we are going to want to save the activation of these examples so we can evaluate the gradient\n",
    "        and back propogate the error to update the weights\n",
    "        :return:\n",
    "        '''\n",
    "        ### First Layer\n",
    "        z1 = np.dot(self.x, self.w1) + self.b1\n",
    "        self.a1 = sigmoid(z1)\n",
    "        ### Second Layer\n",
    "        z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "        self.a2 = sigmoid(z2)\n",
    "        ### Third Layer\n",
    "        z3 = np.dot(self.a2, self.w3) + self.b3\n",
    "        self.a3 = softmax(z3)\n",
    "\n",
    "    def backprop(self):\n",
    "        '''\n",
    "        Back propogates the error to update the waits.\n",
    "        This evaluates the error and the propogates the error through the layers that we built in the \n",
    "        initializer. \n",
    "        '''\n",
    "        # evaluate loss\n",
    "        loss = error(self.a3, self.y)\n",
    "\n",
    "        print('Error: ')\n",
    "        print(loss)\n",
    "        \n",
    "        # Find the derivative values using the chain rule \n",
    "        d_a3 = cross_entropy(self.a3, self.y)\n",
    "        d_z2 = np.dot(d_a3, self.w3.T)\n",
    "        d_a2 = d_z2 * dSigmoid_ds(self.a2)\n",
    "        d_z1 = np.dot(d_a2, self.w2.T)\n",
    "        d_a1 = d_z1 * dSigmoid_ds(self.a1)\n",
    "        \n",
    "        # Now update the weights and biases \n",
    "        self.w3 -= self.lr * np.dot(self.a2.T, d_a3)\n",
    "        self.b3 -= self.lr * np.sum(d_a3, axis = 0, keepdims=True)\n",
    "\n",
    "        self.w2 -= self.lr * np.dot(self.a1.T, d_a2)\n",
    "        self.b2 -= self.lr * np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "        self.w1 -= self.lr * np.dot(self.x.T, d_a1)\n",
    "        self.b1 -= self.lr * np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    def predict(self, data):\n",
    "        '''\n",
    "        Finally, in the end we will want our NN to be able to predict on a given test subject. \n",
    "        Therefore, this functino predicts and returns the answer. \n",
    "        '''\n",
    "        self.x = data\n",
    "        self.feedforward()\n",
    "        return self.a3.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Training becomes very different because wee need to control the NN differently than wee did in Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rotation\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: \n",
      "45.027894042362384\n",
      "Homemade NN Accuracy is: 23.599999999999998\n"
     ]
    }
   ],
   "source": [
    "basic_model = BasicNN(x_train, y_train_targ.values)\n",
    "# Let's use the same amount of epochs as before. \n",
    "epochs = 1\n",
    "for i in range(epochs):\n",
    "    print('Epoch: ' + str(i))\n",
    "    basic_model.feedforward()\n",
    "    basic_model.backprop()\n",
    "\n",
    "pred = basic_model.predict(x_val)\n",
    "\n",
    "acc = 0\n",
    "for i in range(pred.shape[0]):\n",
    "    if pred[i] == y_val_targ.values.argmax(axis=1)[i]:\n",
    "        acc += 1\n",
    "\n",
    "print('Homemade NN Accuracy is: ' + str(acc/y_val_targ.values.shape[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this gives us an accuracy for the Homemade NN on the same data we used with the Keras model. \n",
    "\n",
    "## Epoches and Mini-Batches\n",
    "\n",
    "Let's test them out using more than one epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 14s 256us/step - loss: 2.4305 - categorical_accuracy: 0.1162\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 14s 258us/step - loss: 2.3464 - categorical_accuracy: 0.1249\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 13s 248us/step - loss: 2.3053 - categorical_accuracy: 0.1479\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 13s 247us/step - loss: 2.2813 - categorical_accuracy: 0.1760\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 15s 280us/step - loss: 2.2652 - categorical_accuracy: 0.1944\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 17s 322us/step - loss: 2.2528 - categorical_accuracy: 0.2122\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 15s 270us/step - loss: 2.2422 - categorical_accuracy: 0.2312\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 14s 256us/step - loss: 2.2324 - categorical_accuracy: 0.2494\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 14s 253us/step - loss: 2.2228 - categorical_accuracy: 0.2675\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 14s 252us/step - loss: 2.2134 - categorical_accuracy: 0.2867\n",
      "6000/6000 [==============================] - 2s 269us/step\n",
      "\n",
      "categorical_accuracy: 29.47%\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rotation\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: \n",
      "36.37114978638717\n",
      "Epoch: 1\n",
      "Error: \n",
      "61.60994194563323\n",
      "Epoch: 2\n",
      "Error: \n",
      "59.94887222393052\n",
      "Epoch: 3\n",
      "Error: \n",
      "78.62141100885326\n",
      "Epoch: 4\n",
      "Error: \n",
      "73.84099124490432\n",
      "Epoch: 5\n",
      "Error: \n",
      "77.33667005797453\n",
      "Epoch: 6\n",
      "Error: \n",
      "80.73445015978126\n",
      "Epoch: 7\n",
      "Error: \n",
      "61.606020691968155\n",
      "Epoch: 8\n",
      "Error: \n",
      "45.218076598535774\n",
      "Epoch: 9\n",
      "Error: \n",
      "42.464619720084826\n",
      "Homemade NN Accuracy is: 53.93333333333333\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1568, input_dim=x_train.shape[1], activation='sigmoid'))\n",
    "model.add(Dense(1568, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=SGD(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train_targ.values, epochs=10, batch_size=x_train.shape[0])\n",
    "\n",
    "\n",
    "scores = model.evaluate(x_val, y_val_targ.values)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "\n",
    "basic_model = BasicNN(x_train, y_train_targ.values)\n",
    "# Let's use the same amount of epochs as before. \n",
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    print('Epoch: ' + str(i))\n",
    "    basic_model.feedforward()\n",
    "    basic_model.backprop()\n",
    "\n",
    "pred = basic_model.predict(x_val)\n",
    "\n",
    "acc = 0\n",
    "for i in range(pred.shape[0]):\n",
    "    if pred[i] == y_val_targ.values.argmax(axis=1)[i]:\n",
    "        acc += 1\n",
    "\n",
    "print('Homemade NN Accuracy is: ' + str(acc/y_val_targ.values.shape[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as you can see, When we increase the Epochs, the Networks get more accurate at predictiing which number it is looking at. \n",
    "\n",
    "Now let's introduce a new concept to the training. Mini-batches. Instead of updating the weights every epoch, a quicker and more efficient way to train is to update the weights multiple times during an epoch. Our Homemade NN unfortunately is not coded to use mini-batches, therefore we will illustrate this using our Keras model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 1568)              1230880   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1568)              2460192   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                15690     \n",
      "=================================================================\n",
      "Total params: 3,706,762\n",
      "Trainable params: 3,706,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 17s 314us/step - loss: 1.9410 - categorical_accuracy: 0.5436\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 19s 348us/step - loss: 1.3383 - categorical_accuracy: 0.7911\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 18s 328us/step - loss: 0.9937 - categorical_accuracy: 0.8343\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 17s 313us/step - loss: 0.7927 - categorical_accuracy: 0.8549\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 17s 315us/step - loss: 0.6684 - categorical_accuracy: 0.8670\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 17s 313us/step - loss: 0.5855 - categorical_accuracy: 0.8768\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 17s 314us/step - loss: 0.5264 - categorical_accuracy: 0.8839\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 17s 322us/step - loss: 0.4822 - categorical_accuracy: 0.8900\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 18s 330us/step - loss: 0.4478 - categorical_accuracy: 0.8949\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 17s 310us/step - loss: 0.4199 - categorical_accuracy: 0.8985\n",
      "6000/6000 [==============================] - 2s 268us/step\n",
      "\n",
      "categorical_accuracy: 89.82%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1568, input_dim=x_train.shape[1], activation='sigmoid'))\n",
    "model.add(Dense(1568, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=SGD(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train_targ.values, epochs=10, batch_size=int(x_train.shape[0]/100))\n",
    "\n",
    "\n",
    "scores = model.evaluate(x_val, y_val_targ.values)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And This turns out to give the best accuracy out of all the models we've built here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks \n",
    "\n",
    "Just like we discussed in lecture, their is a posibility of not flattening your images and actually using convolutional filter layers to learn features in the image, which better assists in image recognition. Therefore, we will build a CNN with the MNIST data set here. Let's start with a new preprocessing step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Training set images have this shape: (54000, 28, 28, 1)\n",
      "The Training set labels have this shape: (54000, 10)\n",
      "The Validation set images have this shape: (6000, 28, 28, 1)\n",
      "The Validation set labels have this shape: (6000, 10)\n"
     ]
    }
   ],
   "source": [
    "(o_x_train, o_y_train), (o_x_test, o_y_test) = mnist.load_data()\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(o_x_train, o_y_train,\n",
    "                                                  test_size=0.1,\n",
    "                                                  random_state=20)\n",
    "\n",
    "y_train_targ = pd.get_dummies(y_train)\n",
    "y_val_targ = pd.get_dummies(y_val)\n",
    "\n",
    "x_train = x_train[:,:,:,np.newaxis]\n",
    "x_val = x_val[:,:,:,np.newaxis]\n",
    "\n",
    "\n",
    "print('The Training set images have this shape: ' + str(x_train.shape))\n",
    "print('The Training set labels have this shape: ' + str(y_train_targ.shape))\n",
    "print('The Validation set images have this shape: ' + str(x_val.shape))\n",
    "print('The Validation set labels have this shape: ' + str(y_val_targ.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a CNN using the keras library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 4)         40        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 4)         148       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 4)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               12928     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 14,406\n",
      "Trainable params: 14,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 6s 114us/step - loss: 14.5174 - categorical_accuracy: 0.0992\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 14.5167 - categorical_accuracy: 0.0994\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 6s 117us/step - loss: 14.5167 - categorical_accuracy: 0.0994\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 6s 106us/step - loss: 14.5167 - categorical_accuracy: 0.0994\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 6s 116us/step - loss: 14.5167 - categorical_accuracy: 0.0994\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 6s 112us/step - loss: 14.5167 - categorical_accuracy: 0.0994\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 6s 115us/step - loss: 14.5167 - categorical_accuracy: 0.0994\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 6s 105us/step - loss: 14.5167 - categorical_accuracy: 0.0994\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 6s 114us/step - loss: 14.5167 - categorical_accuracy: 0.0994\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 6s 115us/step - loss: 14.5167 - categorical_accuracy: 0.0994\n",
      "6000/6000 [==============================] - 1s 101us/step\n",
      "\n",
      "categorical_accuracy: 9.88%\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(4, kernel_size=(3,3), input_shape=x_train.shape[1::], activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(4, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=SGD(), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train_targ.values, epochs=10, batch_size=int(x_train.shape[0]/100))\n",
    "\n",
    "\n",
    "scores = model.evaluate(x_val, y_val_targ.values)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You;ve made a CNN and it's given an even better accuracy then the previous 2 models. This is the basics for Neural networks and how to use Keras! Now go forth and build NN to help you with your research! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
